\chapter{Writing a Configuration File}
\label{chap:configfile}

\section{General Configuration}
\label{sec:generalconfig}

Referring again to Fig. \ref{fig:workflow}, remember that the default parameters governing the behavior of the simulation are stored in the file ``\textit{configs/default\_config.m}''. This file is loaded automatically by the toolbox before loading the user-defined one. Hence, each setting defined in the default configuration can be adapted by simply redefining it in the user-defined configuration file, which effectively overwrites the default parameter.

Below is a list of the most important parameters defined in the default config file:

\begin{itemize}
\item \verb;nRuns; : the number of times the simulation will be run. As an example, if \verb;nRuns; is set equal to $2$, each test will be performed twice. This defaults to $1$.
\item \verb|partition_strategy|: an object describing how the toolbox should subdivide the data for testing. This can be any object of class \verb|PartitionStrategy|. Currently, two possibilities are available:

\begin{itemize}
\item \verb|partition_strategy| can be an object of class \verb|HoldoutPartition|, in which case a specified fraction of data will be kept for testing (this is called \textit{holdout}). For example, by setting:

\begin{lstlisting}
partition_strategy = HoldoutPartition(0.3);
\end{lstlisting}

each algorithm will be trained on a randomly selected $70\%$ of the data, and tested on the remaining $30\%$.

\item \verb|partition_strategy| can be an object of class \verb|KFoldPartition|, in which case a $k$-fold cross validation is performed. Typical values of folds in this case are between $3$ and $10$.

\end{itemize}


The default behavior is a $3$-fold cross-validation. Note that the partitioning strategy can equivalently be set using the \verb|set_partition_strategy| method.

\item \verb|seed_prng|: the seed for initializing the pseudo-random number  of Matlab. This can be an integer number or the string ``shuffle'' (default value), in which case a random seed is used. In the latter case, the random seed is printed on screen for allowing the repetition of the exact experiment.
\end{itemize}

\noindent Hence, the default configuration comprises the following parameters:

\begin{lstlisting}
nRuns = 1;
partition_strategy = KFoldPartition(3);
seed_prng = 'shuffle';
\end{lstlisting}

\noindent Note that configuration files can be easily nested. As an example, suppose we have a range of configurations (e.g. a set of algorithms or datasets) that we include frequently in our simulations. These can be written on a particular file, e.g. ``\textit{configs/own\_config.m}'', and imported when needed in the standard configuration file:

\begin{lstlisting}
own_config.m;
\end{lstlisting}


\section{Adding an Algorithm}

An algorithm can be added to the simulation with the following syntax:

\begin{console}
add_algorithm(ID, name, pointer, varargin);
\end{console}

\noindent where:

\begin{itemize}
	\item \verb|ID| is a unique string for identifying the algorithm in the simulation,
	\item \verb|name| is the algorithm’s name (to be displayed in the results),
	\item \verb|pointer| is a pointer to the algorithm’s class,
	\item \verb|varargin| are the additional parameters to be passed to the constructor.
\end{itemize}

\noindent As in the standard convention of Matlab coding, additional parameters are composed as follows:

\begin{itemize}
	\item One or more required parameters,
	\item One or more additional parameters,
	\item One or more name/value pairs. As a general rule, most parameters are in this form.
\end{itemize}

\noindent For example, the following call:

\begin{lstlisting}
add_algorithm('NN', 'Neural Net', @MultilayerPerceptron);
\end{lstlisting}

\noindent adds a default-initialized Multilayer Perceptron to the simulation. Similarly, the call:

\begin{lstlisting}
add_algorithm('NN', 'Neural Net', @MultilayerPerceptron, 'hiddenNodes', 15);
\end{lstlisting}

\noindent adds a Multilayer Perceptron, with $15$ nodes in the hidden layer.

A concise HTML report with a list of all implemented algorithms and respective parameters can be found by calling the script ``\textit{help/info\_algorithms.m}''. Additional information on each algorithm can be found by visualizing the help for the respective class.

\section{Adding a Wrapper}

A wrapper provides additional functionalities to an algorithm (called in this context its \textit{base algorithm}) by encapsulating its behavior and intercepting inputs and outputs to its training and testing functions. Wrappers can be used for fine-tuning model parameters, extracting or selecting features, saving the models resulting from the simulation, etc.

The syntax for adding a wrapper is similar to the syntax for adding an algorithm:

\begin{console}
add_wrapper(ID, wrapper, varargin);
\end{console}

\noindent Where:

\begin{itemize}
\item \verb|ID| is the ID of the algorithm to be encapsulated,
\item \verb|wrapper| is a pointer to the wrapper’s class,
\item \verb|varargin| are the additional parameters for the wrapper.
\end{itemize}

\noindent Consider as an example the following call:
\begin{lstlisting}
add_wrapper('NN', @Featuresearch_GA);
\end{lstlisting}

\noindent This adds a wrapper to the Multilayer Perceptron defined in the previous section, that runs a genetic algorithm for searching the optimal subset of features. For details on the implemented wrappers and required parameters, the script ``\textit{help/info\_wrappers.m}'' generates a concise report. More information on each wrapper is available by visualizing the help for the corresponding class.

It is important to note that wrappers can pile on top of each other:

\begin{console}
add_wrapper(ID, wrapper1, ...);
add_wrapper(ID, wrapper2, ...);
\end{console}

In this case, the \textit{wrapper2} is executed by using as a base algorithm the \textit{wrapper1}, which in turn uses as a base class the algorithm identified by ID. Consider the following:

\begin{lstlisting}
add_wrapper('NN', @Featuresearch_GA);
add_wrapper('NN', @OneVersusAll);
\end{lstlisting}

\noindent In this case, for a multiclass classification problems with $M$ classes $M$ different neural networks are trained. Each network will have a different subset of input parameters, found by the genetic search. The order in which the wrappers are stacked is very important. The following call is different from the previous one:

\begin{lstlisting}
add_wrapper('NN', @OneVersusAll);
add_wrapper('NN', @Featuresearch_GA);
\end{lstlisting}

\noindent Here, a single genetic search is executed, and internally the algorithm is trained using a one-versus-all strategy.

Many wrappers require a parameter designating how to subdivide the data for performing a validation step. In this case, this parameter follows the same semantic as the \verb|testParameter| defined for the general configuration.

\subsection{Performing a Parameter Sweep}
\label{sec:parametersweep}

A very common need in supervised learning is testing a given set of values of one or more parameters of an algorithm, then choosing the combination resulting in the highest level of accuracy. In the toolbox, this functionality is provided natively by the \textit{ParameterSweep} wrapper. As an example of its usage, consider the Extreme Learning Machine (ELM) adopted in the sample configuration, whose simulation has been analyzed in the previous chapter. It has two parameters, a regularization factor (denoted by \verb|regularizationFactor|) and a kernel parameter (denoted by \verb|kernel_para|). As is standard practice, we want to test the following range of values for each parameter:

\begin{equation}
2^{-5}, 2^{-4}, \dots, 2^{5}
\end{equation}

\noindent Since we have two parameters, this results in $10 \times 10 = 100$ configurations to be tested. The validation accuracy of each configuration should be computed by performing an inner $3$-fold cross validation on the training data. The following command generates the corresponding wrapper in our example:

\newpage

\begin{lstlisting}
add_wrapper('ELM', @ParameterSweep, 3, {'regularizationFactor', 'kernel_para'}, {'exp', 'exp'}, [-5 5; -5 5], [1 1]);
\end{lstlisting}

\noindent Although this may seem daunting at first, it is actually rather simple. The first parameter has a semantic similar to the \verb|testParameter| discussed in Section \ref{sec:generalconfig}, and it instructs the wrapper to perform a $3$-fold cross validation. This is followed by a cell array with the names of the parameters to be tested. The next cell array tells the wrapper that the values must be computed in an exponential fashion, i.e., by powers of $2$. Then, we provide the lower and upper bounds for each parameter, and the step values for computing the ranges.

\subsection{Saving and Loading a Configuration}

The second common requirement that we analyze here briefly is that of saving the configuration of an algorithm, for retrieving it in a following simulation. As an example, the grid search of the previous subsection requires training and testing $100$ models, each time performing a $3$-fold cross validation. For large datasets, this requires a very large time, hence we would like to save the results for loading it successively. The \textit{SaveConfiguration} wrapper can be used for saving a configuration:

\begin{lstlisting}
add_wrapper('ELM', @SaveConfiguration, 'ELM_SAVED', './sweeps');
\end{lstlisting}

\noindent After training a model, this will be saved in the ``\textit{./sweeps}'' folder. Note that a simulation requires training several models, one for each fold and dataset. Hence, this will save several files in the folder. The naming convention is that a file is denoted as ID\_DATASET\_FOLD.mat, where ID is the id defined above (ELM\_SAVED in our example), DATASET is the name of the dataset, and FOLD is the numerical id of the fold. The saved models can be retrieved on a successive simulation using the \textit{LoadConfiguration} wrapper:

\begin{lstlisting}
add_wrapper('ELM', @LoadConfiguration, './sweeps', 'ELM_SAVED', {'regularizationFactor', 'kernel_para'});
\end{lstlisting}

\noindent Note that this wrapper requires explicitly a list of training parameters to be loaded.

\section{Adding a Dataset}

A new dataset can be added to the simulation with the following syntax:

\begin{console}
add_dataset(ID, name, dataset, [subsample], varargin);
\end{console}

Where:

\begin{itemize}
\item \verb|ID| is a string identifying the dataset in the simulation,
\item \verb|name| is a name for the dataset, to be displayed in the results,
\item \verb|dataset| is the unique alphanumerical string denoting the dataset in the filesystem,
\item \verb|subsample| is a percentage of the dataset to load (optional),
\item \verb|varargin| are additional parameters to be given in specific cases.
\end{itemize}

\noindent As an example, the call:

\begin{lstlisting}
add_dataset('Y', 'Yacht', 'uci_yacht');
\end{lstlisting}

\noindent adds the dataset ``\textit{uci\_yacht}'' to the simulation with name ``Yacht'', while:

\begin{lstlisting}
add_dataset('Y', 'Yacht', 'uci_yacht', 0.5);
\end{lstlisting}

\noindent adds the same dataset, but loads only a randomly chosen 50\% of it. A list of available datasets (and respective ids) can be obtained by calling ``\textit{help/info\_datasets.m}''. The datasets are divided depending on the task. Below we provide more information on the way in which each complex task can be loaded.

\subsection{Adding a Prediction Dataset}

In the case of a prediction task, the toolbox requires the embedding dimension for the input vector:

\begin{lstlisting}
add_dataset('MG', 'Mackey-Glass', 'mackeyglass', 1, 'embeddingFactor', 7);
\end{lstlisting}

\noindent Note that in this case it is necessary to specify a subsampling percentage.

\subsection{Adding a Multi-label Dataset}
\label{sec:chap2:multilabeldataset}

Remember from Section \ref{sec:understandingtasks} that a multi-label task with $M$ labels is loaded as $M$ different binary classification tasks. The name of each of these sub-tasks is created from the name provided by the user and an additional string contained in the dataset. The id, instead, is created sequentially starting from the user-defined id. As an example, suppose \textit{ml\_example} is a multi-label dataset consisting of $3$ labels. Consider the following call:

\begin{lstlisting}
add_dataset('EX', 'Multi-Label Dataset', 'ml_example');
\end{lstlisting}

\noindent This call creates $3$ distinct binary classification tasks, with ids given by EX-1, EX-2, and EX-3.

\section{Adding a Preprocessor to a Dataset}

A preprocessor applies some specific transformation to a dataset in the initialization phase. A preprocessor can be added with the following syntax:

\begin{console}
add_preprocessor(ID, preprocessor, varargin);
\end{console}

\noindent Where:

\begin{itemize}
\item \verb|ID| is a regular expression that should match with all the datasets ids to which the preprocessor should be applied,
\item \verb|preprocessor| is a pointer to the preprocessor’s class,
\item \verb|varargin| are the additional parameters of the preprocessor.
\end{itemize}

\noindent As an example, the call:

\begin{lstlisting}
add_preprocessor('Y', @ApplyPca, 'varianceToPreserve', 0.95);
\end{lstlisting}

\noindent applies a PCA transformation to the dataset previously defined, retaining only the principal components encompassing at least 95\% of the variance of the original input.

A list of the available preprocessors is obtained by calling the script ``\textit{help/info\_preprocessors.m}''. Additional information on each preprocessor is given by the help of the corresponding class.

\subsection{Preprocessors and Multi-label Tasks}

The fact that the syntax for adding a pre-processor accepts a regular expressions is particularly suited for multi-label tasks. Continuing the example of Section \ref{sec:chap2:multilabeldataset}, suppose we now want to perform a PCA on all three binary classification tasks. This can be done with a single call:

\begin{lstlisting}
add_preprocessor('EX-.', @ApplyPca, 'varianceToPreserve', 0.95);
\end{lstlisting}

\noindent The regular expression follows the standard Matlab convention: in particular, the dot refers to ``any character'', allowing to match all three datasets together. More information on the syntax of regular expressions can be found by reading the help of the built-in ``\textit{regexp}'' function.

\section{Choosing a Statistical Test}

A statistical testing is requested by calling the following method:

\begin{lstlisting}
set_statistical_test(class);
\end{lstlisting}

\noindent where \verb|class| is a pointer to a class deriving from \verb|StatisticalTest|. Each statistical test has certain conditions that must be satisfied so that it can be called. As an example, the Wilcoxon signed-rank text requires exactly two algorithms and at least two datasets. So, if we add it to a simulation like:

\begin{lstlisting}
set_statistical_test(@WilcoxonTest);
\end{lstlisting}

\noindent but we do not respect the conditions, a critical error is issued during the initialization phase.